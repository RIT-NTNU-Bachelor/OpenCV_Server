{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Comparing Accuracy for Real-Time Face Detection Models\n",
    "\n",
    "The goal of this case study is to figure out what the best model would be for real-time face detection. More specifically, what model detect images more accurately. Instead of using a video, this study use a set of images instead of videos. The model will only be robust if it is tested on a set of images that are diverse in lighting condition, position of the faces, amount of faces, ski color, etc. Videos is essentially a finite set of frames (images) put together. The argument is that a diverse set of images will test the accuracy better than a video would do. <br>\n",
    "\n",
    "The face detection models compared will be: \n",
    "- Haar Cascade\n",
    "- HOG\n",
    "- DNN\n",
    "- MMOD\n",
    "\n",
    "All of the tested videos are within the folder `data/test_data/` in the same repository. Run the following code block to import all libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all packages\n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# For benchmarking the process of generating the graph\n",
    "import time\n",
    "\n",
    "# For creating the progress bar when looping over the dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Code snippets for each model\n",
    "\n",
    "The following code are the function for each face detection model. The code for the individual models can be found in within `src/models/code/` the `OpenCV_Server` repository. Each file has a function that is called on a single frame. The output is either a list of boundary boxes (`Rect`) or `None`. The functions are moved over to this file for the sake of simplicity.\n",
    "\n",
    "Run them to save the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haar Cascade\n",
    "def detect_face_haar(img,detectMultipleFaces=False, scale=1.1, neighbors=10, size=50):\n",
    "    \"\"\"Detect a face in an image using a pre-trained Haar Cascade model. \n",
    "\n",
    "    The model has been trained by OpenCV.\n",
    "    See: https://opencv.org/\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): \n",
    "            Image read from the cv2.imread function. Ut is a numpy\n",
    "        detectMultipleFaces (boolean): \n",
    "            Toggle for returning more than one face detected. Default is false. \n",
    "        scale (float, optional): \n",
    "            For scaling down the input image, before trying to detect a face. Makes it easier to detect a face with smaller scale. Defaults to 1.1.\n",
    "        neighbors (int, optional): \n",
    "            Amount of neighbor rectangles needed for a face to be set as detected. Defaults to 10.\n",
    "        size (int, optional): \n",
    "            Size of the sliding window that checks for any facial features. Should match the face size in the image, that should be detected. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        Rect: Datatype of a rectangle, that overlays the position of the detected face. It has four attributes of intrests: x-position, y-position, \n",
    "    \"\"\"\n",
    "\n",
    "    # Turing the image into a grayscale image\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Printing the gray scale image\n",
    "    # print(f\"Gray-Scale Image dimension: ({gray_image.shape})\")\n",
    "\n",
    "    # Loading the classifier from a pretrained dataset\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    )\n",
    "\n",
    "    # Performing the face detection\n",
    "    faces = face_classifier.detectMultiScale(\n",
    "        gray_image, scaleFactor=scale, minNeighbors=neighbors, minSize=(size,size)\n",
    "    )\n",
    "\n",
    "    # Return amount of \n",
    "    if detectMultipleFaces == True:\n",
    "        return faces\n",
    "    return faces[0]\n",
    "\n",
    "\n",
    "# HOG\n",
    "def detect_face_hog(img,detectMultipleFaces=False):\n",
    "    # Turing the image into a grayscale image\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Define the HOG detector from dlib\n",
    "    hog_face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Detect faces from the grayscale image\n",
    "    faces = hog_face_detector(gray_image, 1)\n",
    "\n",
    "    # Check if only one face is needed\n",
    "    if detectMultipleFaces == False:\n",
    "        return faces[0]\n",
    "\n",
    "    return faces\n",
    "\n",
    "\n",
    "# DNN\n",
    "def detect_face_dnn(img, net, framework=\"caffe\", conf_threshold=0.7, detectMultipleFaces=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using a deep neural network (DNN).\n",
    "\n",
    "    Parameters:\n",
    "    - img: The input image.\n",
    "    - net: The pre-trained DNN model for face detection.\n",
    "    - framework: The framework used for the DNN model (\"caffe\" or \"tensorflow\").\n",
    "    - conf_threshold: The confidence threshold for detecting faces.\n",
    "    - detect_multiple_faces: Boolean flag to detect multiple faces or just the first one.\n",
    "\n",
    "    Returns:\n",
    "    - A list of bounding boxes for detected faces or a single bounding box if detectMultipleFaces is False.\n",
    "    \"\"\"\n",
    "    frameHeight = img.shape[0]\n",
    "    frameWidth = img.shape[1]\n",
    "    if framework == \"caffe\":\n",
    "        blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    else:\n",
    "        blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            bboxes.append((x1, y1, width, height))\n",
    "\n",
    "    if detectMultipleFaces == True:\n",
    "        return bboxes  # Return all detected faces\n",
    "    else:\n",
    "        return bboxes[0] if bboxes else None # Return the first face or None if no faces are detected\n",
    "    \n",
    "\n",
    "# MMOD \n",
    "def detect_face_mmod(img, detector, inHeight=300, inWidth=0, detectMultipleFaces=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using the dlib MMOD detector.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The input image.\n",
    "    - detector: The dlib MMOD face detector.\n",
    "    - inHeight: The height of the image for detection.\n",
    "    - inWidth: The width of the image for detection. If 0, it will be calculated based on the aspect ratio of the input image.\n",
    "    - detectMultipleFaces: Boolean flag to indicate whether to detect multiple faces or just the first one.\n",
    "\n",
    "    Returns:\n",
    "    - A list of bounding boxes for each detected face or a single bounding box if detectMultipleFaces is False.\n",
    "    \"\"\"\n",
    "    frameHeight = img.shape[0]\n",
    "    frameWidth = img.shape[1]\n",
    "    if not inWidth:\n",
    "        inWidth = int((frameWidth / frameHeight) * inHeight)\n",
    "\n",
    "    scaleHeight = frameHeight / inHeight\n",
    "    scaleWidth = frameWidth / inWidth\n",
    "\n",
    "    resized_img = cv2.resize(img, (inWidth, inHeight))\n",
    "    resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "    faceRects = detector(resized_img, 0)\n",
    "\n",
    "    bboxes = []\n",
    "    for faceRect in faceRects:\n",
    "        x1 = int(faceRect.rect.left() * scaleWidth)\n",
    "        y1 = int(faceRect.rect.top() * scaleHeight)\n",
    "        x2 = int(faceRect.rect.right() * scaleWidth)\n",
    "        y2 = int(faceRect.rect.bottom() * scaleHeight)\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        bboxes.append((x1, y1, width, height))\n",
    "\n",
    "    if detectMultipleFaces == True:\n",
    "        return bboxes  # Return all detected faces\n",
    "    else:\n",
    "        return bboxes[0] if bboxes else None  # Return the first face or None if no faces are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metrics for quantifying face detection accuracy  \n",
    "\n",
    "The metrics for face detection accuracy have been the same as any object detection problem: \n",
    "\n",
    "**IoU:** Intersection of Union is the relationship between the predicted boundary box and the actual boundary box. <br>\n",
    "\n",
    "**Precision:** Predicted positives that are correct. <br>\n",
    "\n",
    "**Recall:** The proportion of predicted ^\n",
    "\n",
    "TODO: REWRITE THIS\n",
    "\n",
    "\n",
    "For this case study, the metrics will be precision as a metric. This requires a dataset that is labeled with images  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Dataset\n",
    "\n",
    "For the accuracy test we are going to use a dataset with 5171 faces in a set of 2845 images. The dataset is called FDDB: Face Detection Data Set and Benchmark, and was created by the University of Massachusetts. A link to the dataset can be found in the [resources](#resources). \n",
    "\n",
    "\n",
    "The images has to be extracted in the path `data/test_data/FDDB/`. This folder is git ignored, meaning that setup is required before running this Jupiter Hub file. Create a new directory within the `data/test_data`, called `FDDB`. This is a huge dataset with over 28 000 images. Not all where used in the FDDB study. The complete list of all images used in the study is located in the file: `data/test_data/fddb_paths.txt`. The file has the paths for the 2845 images that were used in the study.\n",
    "\n",
    "\n",
    "To test that the dataset has been correctly imported run the following code blocks (the image is of a golfer with a golf club): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect 2845 images\n",
    "total_amount_of_images = 2845\n",
    "total_amount_of_faces = 5171\n",
    "actual = 0\n",
    "\n",
    "# Count how many lines there is in the list of all images used by the \n",
    "with open(\"../data/test_data/fddb_paths.txt\", \"r\") as file:\n",
    "    actual = sum(1 for line in file)\n",
    "\n",
    "# Assert that this is the case\n",
    "assert actual == total_amount_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the first image in the dataset \n",
    "path = \"../data/test_data/FDDB/2002/07/19/big/img_65.jpg\"\n",
    "\n",
    "# Opening the image and reading the image for the correct model\n",
    "img = cv2.imread(path)\n",
    "img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Showing the image (should be no error)\n",
    "pyplot.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparing the models \n",
    "\n",
    "We....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of \n",
    "dataset_folder = \"../data/test_data/FDDB\"\n",
    "\n",
    "# Variable for counting how many images was processed \n",
    "image_count = 0\n",
    "\n",
    "# Start benchmarking \n",
    "start_time = time.time()\n",
    "\n",
    "# Variables for the data\n",
    "haar_positives = 0\n",
    "hog_positives = 0\n",
    "dnn_positives = 0 \n",
    "\n",
    "# Setup NET for DNN\n",
    "modelFile = \"../src/models/trained_models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "configFile = \"../src/models/trained_models/deploy.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "# Total images for this dataset \n",
    "total_amount_of_images = 2845\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_amount_of_images, desc=\"Processing Images\")\n",
    "\n",
    "\n",
    "# Walk through all directories and files in the folder\n",
    "with open(\"../data/test_data/fddb_paths.txt\", \"r\") as file:\n",
    "    for path in file: \n",
    "        # Increment the image count\n",
    "        image_count += 1\n",
    "\n",
    "        # Full path \n",
    "        image_path = \"../data/test_data/FDDB/\" + path.strip() + \".jpg\"\n",
    "\n",
    "        # Read image and correct color\n",
    "        img = cv2.imread(image_path) \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Test Haar Method \n",
    "        faces_haar = detect_face_haar(img, True)\n",
    "        if faces_haar is not None and len(faces_haar) > 0:\n",
    "            haar_positives += len(faces_haar) \n",
    "\n",
    "        # Test HOG Method \n",
    "        faces_hog = detect_face_hog(img, True)\n",
    "        if faces_hog is not None and len(faces_hog) > 0:\n",
    "            hog_positives += len(faces_hog)\n",
    "\n",
    "        # Test DNN Method\n",
    "        faces_dnn = detect_face_dnn(img, net)\n",
    "        if faces_dnn is not None and len(faces_dnn) > 0:\n",
    "            dnn_positives += len(faces_dnn)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Stop the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# End benchmarking time\n",
    "end_time = time.time()\n",
    "seconds_passed = round(end_time - start_time, 0)\n",
    "min, sec = divmod(seconds_passed, 60)\n",
    "\n",
    "\n",
    "# Print metrics of process \n",
    "print(f\"Images processed: {image_count}\")\n",
    "print(f\"Time elapsed:   {min} min, {sec}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the result **R** as a percentage (rounded to two decimals): \n",
    "$$\n",
    "  R = \\frac{\\text{Amount of detected faces}}{\\text{Total amount of faces}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the percentage of each individual method \n",
    "haar_res = round((haar_positives / total_amount_of_faces) * 100, 2)\n",
    "hog_res = round((hog_positives / total_amount_of_faces) * 100, 2)\n",
    "dnn_res = round((dnn_positives / total_amount_of_faces) * 100, 2)\n",
    "\n",
    "# Printing the result\n",
    "print(f\"HAAR: {haar_res}% of faces detected \")\n",
    "print(f\"HOG: {hog_res}% of faces detected \")\n",
    "print(f\"DNN: {dnn_res}% of faces detected \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `matplotlib` to plot the data in a horizontal bar graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up labels and values to plot (x, y axis labels)\n",
    "methods = [\"HAAR\", \"HOG\", \"DNN\"]\n",
    "percentages = [haar_res, hog_res, dnn_res]\n",
    "\n",
    "# Creating the horizontal bar chart\n",
    "pyplot.figure(figsize=(10, 6))\n",
    "bars = pyplot.barh(methods, percentages, color=[\"blue\", \"orange\", \"green\"])\n",
    "\n",
    "# Adding the text labels on the bars\n",
    "for bar in bars:\n",
    "    # Set labels to the left of the bar\n",
    "    width = bar.get_width()\n",
    "    label_x_pos = width + 1 \n",
    "\n",
    "    # Plotting the bar\n",
    "    pyplot.text(label_x_pos, bar.get_y() + bar.get_height()/2, f\"{width}%\", va=\"center\")\n",
    "\n",
    "# Setting the labels and title\n",
    "pyplot.xlabel(f\"Percentage of Faces Detected\\n\\nFaces labeled = {total_amount_of_faces}, Images = {total_amount_of_images}\")\n",
    "pyplot.title(\"Comparing Face Detection Methods (Accuracy)\")\n",
    "\n",
    "# Adjust the x-axis limits if you want more space for text\n",
    "pyplot.xlim(0, max(percentages) + 10)  # Adding 10 for a bit of extra space on the right\n",
    "\n",
    "# Saving the figure to a set path \n",
    "pyplot.savefig(\"../data/results/compare_face_detection_model_accuracy.png\")\n",
    "\n",
    "# Showing the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources and Credits: \n",
    "\n",
    "Metrics for face detection model comparison: <br>\n",
    "https://learnopencv.com/what-is-face-detection-the-ultimate-guide/#Metrics-used-for-Face-Detection <br>\n",
    "\n",
    "The FDDB Dataset with for benchmarking: <br>\n",
    "http://vis-www.cs.umass.edu/fddb/ \n",
    "\n",
    "\n",
    "Check out the GitHub repository here: [GitHub Repository](https://github.com/RIT-NTNU-Bachelor/OpenCV_Server)\n",
    "\n",
    "**Created by:** Kjetil Indrehus, Sander Hauge and Martin Johannessen\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
