{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Comparing Efficacy for Real-Time Face Detection Models\n",
    "\n",
    "A crucial factor for success when it comes to real-time face detection system is efficacy.\n",
    "In this case study, we compare different methods for face detection and compare their efficacy.\n",
    "The methods are:\n",
    "\n",
    "- Hog\n",
    "- Haar\n",
    "- DNN\n",
    "\n",
    "Run the following code block to setup imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for CV2, DLIB and Plotting \n",
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# For benchmarking the process of generating the graph\n",
    "import time\n",
    "\n",
    "# For creating the progress bar when looping over the dataset\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Code snippets for each model\n",
    "\n",
    "The following code are the function for each face detection model. The code for the individual models can be found in within `src/models/code/` the `OpenCV_Server` repository. Each file has a function that is called on a single frame. The output is either a list of boundary boxes (`Rect`) or `None`. The functions are moved over to this file for the sake of simplicity.\n",
    "\n",
    "Run them to save the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haar Cascade\n",
    "def detect_face_haar(img,detectMultipleFaces=False, scale=1.1, neighbors=10, size=50):\n",
    "    \"\"\"Detect a face in an image using a pre-trained Haar Cascade model. \n",
    "\n",
    "    The model has been trained by OpenCV.\n",
    "    See: https://opencv.org/\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): \n",
    "            Image read from the cv2.imread function. Ut is a numpy\n",
    "        detectMultipleFaces (boolean): \n",
    "            Toggle for returning more than one face detected. Default is false. \n",
    "        scale (float, optional): \n",
    "            For scaling down the input image, before trying to detect a face. Makes it easier to detect a face with smaller scale. Defaults to 1.1.\n",
    "        neighbors (int, optional): \n",
    "            Amount of neighbor rectangles needed for a face to be set as detected. Defaults to 10.\n",
    "        size (int, optional): \n",
    "            Size of the sliding window that checks for any facial features. Should match the face size in the image, that should be detected. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        Rect: Datatype of a rectangle, that overlays the position of the detected face. It has four attributes of intrests: x-position, y-position, \n",
    "    \"\"\"\n",
    "\n",
    "    # Turing the image into a grayscale image\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Printing the gray scale image\n",
    "    # print(f\"Gray-Scale Image dimension: ({gray_image.shape})\")\n",
    "\n",
    "    # Loading the classifier from a pretrained dataset\n",
    "    face_classifier = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    )\n",
    "\n",
    "    # Performing the face detection\n",
    "    faces = face_classifier.detectMultiScale(\n",
    "        gray_image, scaleFactor=scale, minNeighbors=neighbors, minSize=(size,size)\n",
    "    )\n",
    "\n",
    "    # Return amount of \n",
    "    if detectMultipleFaces == True:\n",
    "        return faces\n",
    "    return faces[0]\n",
    "\n",
    "\n",
    "# HOG\n",
    "def detect_face_hog(img,detectMultipleFaces=False):\n",
    "    # Turing the image into a grayscale image\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Define the HOG detector from dlib\n",
    "    hog_face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Detect faces from the grayscale image\n",
    "    faces = hog_face_detector(gray_image, 1)\n",
    "\n",
    "    # Check if only one face is needed\n",
    "    if detectMultipleFaces == False:\n",
    "        return faces[0]\n",
    "\n",
    "    return faces\n",
    "\n",
    "\n",
    "# DNN\n",
    "def detect_face_dnn(img, net, framework=\"caffe\", conf_threshold=0.7, detectMultipleFaces=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using a deep neural network (DNN).\n",
    "\n",
    "    Parameters:\n",
    "    - img: The input image.\n",
    "    - net: The pre-trained DNN model for face detection.\n",
    "    - framework: The framework used for the DNN model (\"caffe\" or \"tensorflow\").\n",
    "    - conf_threshold: The confidence threshold for detecting faces.\n",
    "    - detect_multiple_faces: Boolean flag to detect multiple faces or just the first one.\n",
    "\n",
    "    Returns:\n",
    "    - A list of bounding boxes for detected faces or a single bounding box if detectMultipleFaces is False.\n",
    "    \"\"\"\n",
    "    frameHeight = img.shape[0]\n",
    "    frameWidth = img.shape[1]\n",
    "    if framework == \"caffe\":\n",
    "        blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    else:\n",
    "        blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            bboxes.append((x1, y1, width, height))\n",
    "\n",
    "    if detectMultipleFaces == True:\n",
    "        return bboxes  # Return all detected faces\n",
    "    else:\n",
    "        return bboxes[0] if bboxes else None # Return the first face or None if no faces are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The DataSet \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TEST THAT DATASET CAN BE LOADED IN\n",
    "\n",
    "# Path to videos \n",
    "video_path = \"../data/test_data/videos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Metrics\n",
    "\n",
    "This case study is going to measure two things: \n",
    "\n",
    "- FPS \n",
    "- Memory usage \n",
    "\n",
    "FPS, because it measures the amount of frames processed by each model. The other metric is Memory usage, because we want to see how memory heavy the algorithms are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Measuring FPS\n",
    "\n",
    "### TODO: WRITE THIS\n",
    "\n",
    "\n",
    "Run th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Measuring Memory Usage\n",
    "\n",
    "### TODO: WRITE THIS\n",
    "\n",
    "\n",
    "Run th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plotting the result in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
