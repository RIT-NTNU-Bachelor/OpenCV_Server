{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenCV_Server Documentation","text":"<p>This documentation provides detailed information about the OpenCV_Server project, a robust solution for computer vision applications leveraging OpenCV. For comprehensive information on MkDocs, visit mkdocs.org.</p>"},{"location":"#project-layout","title":"Project Layout","text":"<p>Here's a brief overview of the project's structure:</p> <pre><code>OpenCV_Server/\n\u2502\n\u251c\u2500 docs/ # The documentation homepage.\n\u251c\u2500 src/ # Source files for the project.\n\u251c\u2500 analysis/ # Jupiter notebooks for our case studies \n\u251c\u2500 scripts/ # Other useful independent scripts\n\u251c\u2500 data/ \n|  \u251c\u2500 results/ # Images from successful unit test, and graphs from case studies \n|  \u2514\u2500 test_data/  # The folder with datasets for the case studies\n\u251c\u2500 data/\n\u251c\u2500 tests/ # Test scripts and test cases.\n\u251c\u2500 mkdocs.yml # The configuration file for MkDocs.\n\u251c\u2500 requirements.txt # Project dependencies.\n\u2514\u2500 ... # And more! \n</code></pre>"},{"location":"cvzone/","title":"CVZone Model Documentation","text":"<p>Documentation for the CVZone-related code.</p> <p>Link to the library can be found here.</p>"},{"location":"cvzone/#src.models.code.cvzone.detect_face_cvzone","title":"<code>detect_face_cvzone(img, detector, detect_multiple_faces=True)</code>","text":"<p>Function that detects faces in an image using the CVZone library. </p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image in which to detect face. Retrieved by OpenCVs imread function.  </p> required <code>detector</code> <code>FaceMeshDetector</code> <p>An instance of the Face Mesh detector, pretrained from the CVZone library</p> required <code>detect_multiple_faces</code> <code>bool</code> <p>Will return multiple faces if true, else only one. Default is set to true. </p> <code>True</code> <p>Returns:</p> Type Description <code>list | tuple | None</code> <p>If <code>detect_multiple_faces</code> is true, returns a list of tuples (x, y, width, height) for each detected face.         Else returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected.         Each tuple represents the top-left corner and dimensions of the bounding box around the detected face.</p> <p>Each tuple contains the coordinates of the top left corner and the dimensions of the bounding box.</p> Source code in <code>src/models/code/cvzone.py</code> <pre><code>def detect_face_cvzone(img: np.ndarray, detector: FaceMeshDetector, detect_multiple_faces=True) -&gt; (list | tuple | None):\n    \"\"\" Function that detects faces in an image using the CVZone library. \n\n    Parameters:\n        img (np.ndarray): The image in which to detect face. Retrieved by OpenCVs imread function.  \n        detector (FaceMeshDetector): An instance of the Face Mesh detector, pretrained from the CVZone library\n        detect_multiple_faces (bool, optional): Will return multiple faces if true, else only one. Default is set to true. \n\n    Returns:\n        If `detect_multiple_faces` is true, returns a list of tuples (x, y, width, height) for each detected face. \\\n        Else returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected. \\\n        Each tuple represents the top-left corner and dimensions of the bounding box around the detected face.\n\n    Each tuple contains the coordinates of the top left corner and the dimensions of the bounding box.\n    \"\"\"\n    _, faces = detector.findFaceMesh(img, draw=False)\n\n    if detect_multiple_faces == True:\n        return faces\n    return faces[0] if faces else None\n</code></pre>"},{"location":"depth/","title":"Estimating Depth","text":"<p>For tracking the depth, the following function is implemented. Depth is the distance between the user and the camera. There is a way to calculate depth if you know the focal length of the camera and the average distance between the eyes. </p>"},{"location":"depth/#src.depth.estimate_depth","title":"<code>estimate_depth(landmarks)</code>","text":"<p>Estimate the Z-coordinate (depth) for a detected face.</p> <p>This function calculates the depth, which is the distance between the screen and the user, using a method that relies on the distance between the eyes.  It uses the focal length and the average distance between the eyes, to estimate the depth based on eye landmarks detected. </p> <p>Parameters:</p> Name Type Description Default <code>landmarks</code> <code>list[list[int]]</code> <p>A list of arrays, each array representing a landmark with x and y position of that landmark.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The distance between the user and the camera</p> Source code in <code>src/depth.py</code> <pre><code>def estimate_depth(landmarks: list[list[int]]):\n    \"\"\"Estimate the Z-coordinate (depth) for a detected face.\n\n    This function calculates the depth, which is the distance between the screen and the user, using a method that relies on the distance between the eyes. \n    It uses the focal length and the average distance between the eyes, to estimate the depth based on eye landmarks detected. \n\n    Parameters:\n        landmarks (list[list[int]]): A list of arrays, each array representing a landmark with x and y position of that landmark.\n\n    Returns:\n        int: The distance between the user and the camera \n\n    \"\"\"\n    # Check that the list has the 468 landmarks \n    if len(landmarks) != 468:\n        print(\"ERROR: Invalid length of landmark list expected 468, was {len(landmarks)}\")\n        return None \n\n    # Retrieve the eye indexes \n    left_eye = landmarks[EYE_DISTANCE_INDEX['left_eye']]\n    right_eye = landmarks[EYE_DISTANCE_INDEX['right_eye']]\n\n    # Calculate distance between eyes\n    w, _ = CVZONE_DETECTOR_MAX_ONE.findDistance(left_eye, right_eye)\n\n    # Estimate depth\n    return int((INTEROCULAR_DISTANCE * FOCAL_LENGTH) / w)\n</code></pre>"},{"location":"dnn/","title":"DNN Model Documentation","text":"<p>Documentation for the Deep Neural Network (DNN) models.</p> <p>For more information on the DNN module in OpenCV, visit OpenCV DNN documentation.</p> <p>See also the following pages for more information about the models  - Caffe Models  - TensorFlow Models </p>"},{"location":"dnn/#src.models.code.dnn.detect_face_dnn","title":"<code>detect_face_dnn(img, net, framework='caffe', conf_threshold=0.7, detect_multiple_faces=False)</code>","text":"<p>Function that detects faces in an image using a Deep Neural Network (DNN) model. The function supports models trained with either the Caffe or TensorFlow framework.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input image in which faces are to be detected. The image should be in a format               acceptable by OpenCV, typically a numpy ndarray obtained from <code>cv2.imread</code>.</p> required <code>net</code> <code>dnn_Net</code> <p>The pre-trained DNN model loaded using <code>cv2.dnn.readNet</code> for face detection.                 For more information on loading models, refer to the OpenCV documentation on dnn_Net.</p> required <code>framework</code> <code>str</code> <p>Specifies the framework of the pre-trained model. Can be 'caffe' or                        'tensorflow'. Defaults to 'caffe'.</p> <code>'caffe'</code> <code>conf_threshold</code> <code>float</code> <p>The minimum confidence threshold for a detection to be considered valid.                                Ranges between 0 and 1, with a higher threshold reducing false positives. Defaults to 0.7.</p> <code>0.7</code> <code>detect_multiple_faces</code> <code>bool</code> <p>If True, detects and returns bounding boxes for all detected faces.                                     If False, returns a bounding box for the most prominent face or None if no faces are detected.</p> <code>False</code> <p>Returns:</p> Type Description <code>list | tuple | None</code> <p>If True, returns a list of tuples (x, y, width, height) for each detected face. If False, returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected. Each tuple contains the coordinates of the top-left corner and the dimensions of the bounding box.</p> Note <p>This function requires that the appropriate DNN model files are accessible and properly configured before use.  The OpenCV Server code has this setup by default.</p> Source code in <code>src/models/code/dnn.py</code> <pre><code>def detect_face_dnn(img: np.ndarray, net: cv2.dnn_Net, framework: str = \"caffe\", conf_threshold: float = 0.7, detect_multiple_faces: bool = False) -&gt; (list | tuple | None):\n    \"\"\"Function that detects faces in an image using a Deep Neural Network (DNN) model. The function supports models trained\n    with either the Caffe or TensorFlow framework.\n\n    Parameters:\n        img (np.ndarray): The input image in which faces are to be detected. The image should be in a format\n                          acceptable by OpenCV, typically a numpy ndarray obtained from `cv2.imread`.\n        net (cv2.dnn_Net): The pre-trained DNN model loaded using `cv2.dnn.readNet` for face detection. \n                           For more information on loading models, refer to the [OpenCV documentation on dnn_Net](https://docs.opencv.org/master/db/d30/classcv_1_1dnn_1_1Net.html).\n        framework (str, optional): Specifies the framework of the pre-trained model. Can be 'caffe' or\n                                   'tensorflow'. Defaults to 'caffe'.\n        conf_threshold (float, optional): The minimum confidence threshold for a detection to be considered valid. \n                                          Ranges between 0 and 1, with a higher threshold reducing false positives. Defaults to 0.7.\n        detect_multiple_faces (bool, optional): If True, detects and returns bounding boxes for all detected faces.\n                                                If False, returns a bounding box for the most prominent face or None if no faces are detected.\n\n    Returns:       \n        If True, returns a list of tuples (x, y, width, height) for each detected face. If False, returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected. Each tuple contains the coordinates of the top-left corner and the dimensions of the bounding box.\n\n    Note:\n        This function requires that the appropriate DNN model files are accessible and properly configured before use. \n        The [OpenCV Server](https://github.com/RIT-NTNU-Bachelor/OpenCV_Server) code has this setup by default. \n    \"\"\"\n\n    # Get the dimensions of the input image\n    frameHeight = img.shape[0]\n    frameWidth = img.shape[1]\n\n    # Prepare the blob from the image (input to the DNN)\n    blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), [104, 117, 123], swapRB=(framework != \"caffe\"), crop=False)\n\n    # Set the prepared blob as the input to the network\n    net.setInput(blob)\n\n    # Forward the value through the network \n    # The output of the DNN is the detections made in the image\n    detections = net.forward()\n\n    # Process detections and extract bounding boxes\n    faces = []\n    for i in range(detections.shape[2]):\n        confidence = detections[0, 0, i, 2]\n        if confidence &gt; conf_threshold:\n            x1 = int(detections[0, 0, i, 3] * frameWidth)\n            y1 = int(detections[0, 0, i, 4] * frameHeight)\n            x2 = int(detections[0, 0, i, 5] * frameWidth)\n            y2 = int(detections[0, 0, i, 6] * frameHeight)\n            width = x2 - x1\n            height = y2 - y1\n            faces.append((x1, y1, width, height))\n\n    # Return detected faces based on the detectMultipleFaces flag\n    if detect_multiple_faces:\n        return faces  # Return all detected faces\n    else:\n        return faces[0] if faces else None  # Return the first detected face or None\n</code></pre>"},{"location":"haar/","title":"Haar Cascade Model Documentation","text":"<p>Documentation for Haar Cascade models.</p> <p>See OpenCV documentation for more information here.  Scale, neighbors and size are also explained documentation here. </p>"},{"location":"haar/#src.models.code.haar.detect_face_haar","title":"<code>detect_face_haar(img, detector, detect_multiple_faces=True, scale=1.1, neighbors=10, size=50)</code>","text":"<p>Function for detecting faces in an image using a pre-trained Haar Cascade model provided by OpenCV.</p> <p>This project uses the \"haarcascade_frontalface_default.xml\" model, but the function allows for other cascade classifier. Default values for scaling, neighbors and size of the window are set. By default the detector will detect multiple faces. Set <code>detect_multiple_faces</code> to false for detecting one face. </p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image in which faces are to be detected, typically obtained from <code>cv2.imread</code>.</p> required <code>detector</code> <code>CascadeClassifier</code> <p>An instance of Haar Cascade detector, pre-trained for face detection.</p> required <code>detect_multiple_faces</code> <code>bool</code> <p>Controls whether to detect multiple faces or just the most prominent one.                                     Defaults to True, detecting multiple faces.</p> <code>True</code> <code>scale</code> <code>float</code> <p>The factor by which the image is scaled down to facilitate detection. Scaling down the image                      can lead to faster detection with less precision. Defaults to 1.1 (10% reduction).</p> <code>1.1</code> <code>neighbors</code> <code>int</code> <p>The number of neighbors each candidate rectangle should have to retain it.                         A higher number gives fewer detections but with higher quality. Defaults to 10.</p> <code>10</code> <code>size</code> <code>int</code> <p>The minimum size of faces to detect, specified as the side length of the square                   sliding window used in detection. Defaults to 50 pixels.</p> <code>50</code> <p>Returns:</p> Type Description <code>list | tuple | None</code> <p>If <code>detect_multiple_faces</code> is true, returns a list of tuples (x, y, width, height) for each detected face.         Else return a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected.         Each tuple represents the top-left corner and dimensions of the bounding box around the detected face.         Each tuple contains the coordinates of the top left corner and the dimensions of the bounding box.</p> Source code in <code>src/models/code/haar.py</code> <pre><code>def detect_face_haar(img: np.ndarray, detector: cv2.CascadeClassifier, detect_multiple_faces: bool = True, scale: float = 1.1, neighbors: int = 10, size: int = 50) -&gt; (list | tuple | None):\n    \"\"\"Function for detecting faces in an image using a pre-trained Haar Cascade model provided by OpenCV.\n\n    This project uses the \"haarcascade_frontalface_default.xml\" model, but the function allows for other cascade classifier.\n    Default values for scaling, neighbors and size of the window are set. By default the detector will detect multiple faces. Set `detect_multiple_faces` to false for detecting one face. \n\n    Parameters:\n        img (np.ndarray): The image in which faces are to be detected, typically obtained from `cv2.imread`.\n        detector (cv2.CascadeClassifier): An instance of Haar Cascade detector, pre-trained for face detection.\n        detect_multiple_faces (bool, optional): Controls whether to detect multiple faces or just the most prominent one.\n                                                Defaults to True, detecting multiple faces.\n        scale (float, optional): The factor by which the image is scaled down to facilitate detection. Scaling down the image\n                                 can lead to faster detection with less precision. Defaults to 1.1 (10% reduction).\n        neighbors (int, optional): The number of neighbors each candidate rectangle should have to retain it. \n                                   A higher number gives fewer detections but with higher quality. Defaults to 10.\n        size (int, optional): The minimum size of faces to detect, specified as the side length of the square\n                              sliding window used in detection. Defaults to 50 pixels.\n\n    Returns:\n        If `detect_multiple_faces` is true, returns a list of tuples (x, y, width, height) for each detected face. \\\n        Else return a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected. \\\n        Each tuple represents the top-left corner and dimensions of the bounding box around the detected face. \\\n        Each tuple contains the coordinates of the top left corner and the dimensions of the bounding box.\n    \"\"\"\n    # Convert the image to a grayscale image to simplify detection\n    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Perform face detection\n    faces = detector.detectMultiScale(\n        gray_image, scaleFactor=scale, minNeighbors=neighbors, minSize=(size, size)\n    )\n\n    # Return either multiple faces or the most prominent one\n    if detect_multiple_faces:\n        return faces\n    else:\n        return faces[0] if len(faces) &gt; 0 else None\n</code></pre>"},{"location":"hog/","title":"HOG Descriptor Model Documentation","text":"<p>Documentation for Histogram of Oriented Gradients (HOG) models. Read the documentation for the model here.</p>"},{"location":"hog/#src.models.code.hog.detect_face_hog","title":"<code>detect_face_hog(img, detector, detect_multiple_faces=False)</code>","text":"<p>Detects faces in an image using dlib's HOG-based face detector. </p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image in which faces are to be detected, typically obtained from <code>cv2.imread</code>.</p> required <code>detector</code> <p>An instance of dlib's HOG-based face detector, typically initialized using <code>dlib.get_frontal_face_detector()</code>.</p> required <code>detect_multiple_faces</code> <code>bool</code> <p>Specifies whether the function should return detections for all faces found                                      (True) or just the most prominent face (False). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>rectangle | rectangles | None</code> <p>If <code>detect_multiple_faces</code> is True, returns a list of <code>dlib.rectangles</code> each indicating a detected face.         If False, returns a single <code>dlib.rectangle</code> for the most prominent face, or None if no faces are detected.         Each <code>dlib.rectangle</code> object represents the bounding box around a detected face with attributes allowing access to the bounding coordinates.</p> Source code in <code>src/models/code/hog.py</code> <pre><code>def detect_face_hog(img: np.ndarray, detector, detect_multiple_faces: bool = False) -&gt; (dlib.rectangle | dlib.rectangles | None):\n    \"\"\"Detects faces in an image using dlib's HOG-based face detector. \n\n\n    Parameters:\n        img (np.ndarray): The image in which faces are to be detected, typically obtained from `cv2.imread`.\n        detector: An instance of dlib's HOG-based face detector, typically initialized using `dlib.get_frontal_face_detector()`.\n        detect_multiple_faces (bool, optional): Specifies whether the function should return detections for all faces found \n                                                (True) or just the most prominent face (False). Defaults to False.\n\n    Returns: \n        If `detect_multiple_faces` is True, returns a list of `dlib.rectangles` each indicating a detected face. \\\n        If False, returns a single `dlib.rectangle` for the most prominent face, or None if no faces are detected. \\\n        Each `dlib.rectangle` object represents the bounding box around a detected face with attributes allowing access to the bounding coordinates. \\\n\n\n    \"\"\"\n    # Convert the image to grayscale to simplify the detection process\n    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    # Perform face detection on the grayscale image\n    faces = detector(gray_image, 1)\n\n    # Handle the return value based on the detectMultipleFaces flag\n    if not detect_multiple_faces:\n        return faces[0] if faces else None\n\n    return faces\n</code></pre>"},{"location":"mmod/","title":"MMOD Model Documentation","text":"<p>Documentation for Max-Margin Object Detection (MMOD) models.</p> <p>Read more about this detection method in the dlib MMOD documentation here.</p>"},{"location":"mmod/#src.models.code.mmod.detect_face_mmod","title":"<code>detect_face_mmod(img, detector, in_height=300, in_width=0, detect_multiple_faces=False)</code>","text":"<p>Detects faces in an image using the CNN-based dlib MMOD (Max-Margin Object Detection) face detector.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The input image in which faces are to be detected. The image should be in a format               acceptable by OpenCV, typically a numpy ndarray obtained from <code>cv2.imread</code>.</p> required <code>detector</code> <code>fhog_object_detector</code> <p>The dlib MMOD face detector object, which can be initialized with                                   <code>dlib.cnn_face_detection_model_v1</code> for CNN models.</p> required <code>in_height</code> <code>int</code> <p>The height to which the image will be resized for detection, maintaining the aspect ratio.                        Default is 300 pixels.</p> <code>300</code> <code>in_width</code> <code>int</code> <p>The width to which the image will be resized for detection. If set to 0, it will be                       calculated based on the aspect ratio of the input image. Defaults to 0.</p> <code>0</code> <code>detect_multiple_faces</code> <code>bool</code> <p>Specifies whether to detect and return bounding boxes for all faces found                                     in the image or just the most prominent face. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list | tuple | None</code> <p>If <code>detect_multiple_faces</code> is true, returns a list of tuples (x, y, width, height) for each detected face.         Else returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected.         Each tuple represents the top-left corner and dimensions of the bounding box around the detected face.</p> Source code in <code>src/models/code/mmod.py</code> <pre><code>def detect_face_mmod(img: np.ndarray, detector: dlib.fhog_object_detector, in_height=300, in_width=0, detect_multiple_faces=False) -&gt; (list | tuple | None) :\n    \"\"\" Detects faces in an image using the CNN-based dlib MMOD (Max-Margin Object Detection) face detector.\n\n    Parameters:\n        img (np.ndarray): The input image in which faces are to be detected. The image should be in a format\n                          acceptable by OpenCV, typically a numpy ndarray obtained from `cv2.imread`.\n        detector (dlib.fhog_object_detector): The dlib MMOD face detector object, which can be initialized with\n                                              `dlib.cnn_face_detection_model_v1` for CNN models.\n        in_height (int, optional): The height to which the image will be resized for detection, maintaining the aspect ratio.\n                                   Default is 300 pixels.\n        in_width (int, optional): The width to which the image will be resized for detection. If set to 0, it will be\n                                  calculated based on the aspect ratio of the input image. Defaults to 0.\n        detect_multiple_faces (bool, optional): Specifies whether to detect and return bounding boxes for all faces found\n                                                in the image or just the most prominent face. Defaults to False.\n\n    Returns:\n        If `detect_multiple_faces` is true, returns a list of tuples (x, y, width, height) for each detected face. \\\n        Else returns a single tuple (x, y, width, height) for the most prominent face, or None if no faces are detected. \\\n        Each tuple represents the top-left corner and dimensions of the bounding box around the detected face.\n\n    \"\"\"\n\n    # Get the dimensions of the input image\n    frame_height = img.shape[0]\n    frame_width = img.shape[1]\n    if not in_width:\n        in_width = int((frame_width / frame_height) * in_height)\n\n    # Calculate the scaling factors for height and width\n    scale_height = frame_height / in_height\n    scale_width = frame_width / in_width\n\n    resized_img = cv2.resize(img, (in_width, in_height))\n    resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n\n    # Perform face detection\n    face_rectangles = detector(resized_img, 0)\n\n    # Process the detected faces and calculate the bounding boxes\n    faces = []\n    for rectangle in face_rectangles:\n        x1 = int(rectangle.rect.left() * scale_width)\n        y1 = int(rectangle.rect.top() * scale_height)\n        x2 = int(rectangle.rect.right() * scale_width)\n        y2 = int(rectangle.rect.bottom() * scale_height)\n        width = x2 - x1\n        height = y2 - y1\n        faces.append((x1, y1, width, height))\n\n    # Return detected faces based on the detectMultipleFaces flag\n    if detect_multiple_faces:\n        return faces  # Return all detected faces\n    else:\n        return faces[0] if faces else None  # Return the first face or None if no faces are detected\n</code></pre>"},{"location":"scripts/","title":"Additional Scripts","text":""},{"location":"scripts/#camera-initialization-and-configuration-value-calculation","title":"Camera Initialization and Configuration Value Calculation","text":"<p>This script initializes a camera, captures a frame, and calculates central coordinates for configuration in the Unreal Engine Client</p>"},{"location":"scripts/#script-overview","title":"Script Overview","text":"<ul> <li>Camera Initialization: Opens the camera to capture images.</li> <li>Image Capture: Attempts to capture a single frame.</li> <li>Error Handling: Verifies camera availability and successful image capture.</li> <li>Configuration Values: Calculates and prints the center coordinates of the image for Unreal Engine configuration.</li> </ul> <p>The configured values that you retrive and need to add to the Client is:</p> <ul> <li>CX: Center of the frame along the x axis</li> <li>CY: Center of the frame along the y axis</li> </ul>"},{"location":"udp_server/","title":"UDP Server Documentation","text":"<p>Documentation for the UDP server used in the project. This projects send the tracking data of the users face over udp.  Read more about the UDP protocol here.</p>"},{"location":"udp_server/#src.udp_server.send_udp_data","title":"<code>send_udp_data(sock, server_address_port, data, log=False)</code>","text":"<p>Sends data over UDP to a specified server and port.</p> <p>Parameters:</p> Name Type Description Default <code>sock</code> <code>socket</code> <p>The socket object used for UDP communication.</p> required <code>server_address_port</code> <code>tuple</code> <p>A tuple containing the server address and port, formatted as (address, port).</p> required <code>data</code> <code>Any</code> <p>The data to be sent over UDP. This can be any data type that is serializable and supported by the socket.</p> required <code>log</code> <code>bool</code> <p>If set to True, the function logs the details of the sent data to the console. Defaults to False.</p> <code>False</code> Note <p>Ensure that the socket ip and port is set to match the Unreal Engine client ip and port.  See the code for the Unreal Engine Client code here.</p> <p>Default ip and port of the client is (127.0.0.1, 5052)</p> Source code in <code>src/udp_server.py</code> <pre><code>def send_udp_data(sock:socket, server_address_port: tuple, data, log=False):\n    \"\"\"Sends data over UDP to a specified server and port.\n\n    Parameters:\n        sock (socket.socket): The socket object used for UDP communication.\n        server_address_port (tuple): A tuple containing the server address and port, formatted as (address, port).\n        data (Any): The data to be sent over UDP. This can be any data type that is serializable and supported by the socket.\n        log (bool): If set to True, the function logs the details of the sent data to the console. Defaults to False.\n\n    Note:\n        Ensure that the socket ip and port is set to match the Unreal Engine client ip and port. \n        See the code for the [Unreal Engine Client code here.](https://github.com/RIT-NTNU-Bachelor/Unreal-facetracking-client)\n\n        Default ip and port of the client is (127.0.0.1, 5052)\n    \"\"\"\n    encoded_data = str.encode(str(data))\n    sock.sendto(encoded_data, server_address_port)\n\n    if log:\n        print(f\"[INFO] Sent {encoded_data} to {server_address_port}\")\n</code></pre>"}]}